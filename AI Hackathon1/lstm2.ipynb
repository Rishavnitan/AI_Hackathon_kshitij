{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183af698",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/kw/2mknhk896xn3vj_txfhlh5v40000gn/T/ipykernel_44149/3294785398.py:33: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  train_df = pd.read_csv(\"train_FD001.txt\", delim_whitespace=True, header=None, names=cols)\n",
      "/var/folders/kw/2mknhk896xn3vj_txfhlh5v40000gn/T/ipykernel_44149/3294785398.py:34: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  test_df  = pd.read_csv(\"test_FD001.txt\", delim_whitespace=True, header=None, names=cols)\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.20696624270382805 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4129009995707714 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.34093888578967124 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4568258239836259 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2120999556489349 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1352719494607093 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.24434922427727482 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.17356155270700238 and num_layers=1\n",
      "  warnings.warn(\n",
      "/Users/rishavkundu/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1443994768619172 and num_layers=1\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "num_layers must be greater than zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 203\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# RUN GWO\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# ============================================================\u001b[39;00m\n\u001b[1;32m    196\u001b[0m bounds \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    197\u001b[0m     (\u001b[38;5;241m32\u001b[39m, \u001b[38;5;241m128\u001b[39m),   \u001b[38;5;66;03m# hidden_dim\u001b[39;00m\n\u001b[1;32m    198\u001b[0m     (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m),      \u001b[38;5;66;03m# num_layers\u001b[39;00m\n\u001b[1;32m    199\u001b[0m     (\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m),  \u001b[38;5;66;03m# dropout\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     (\u001b[38;5;241m1e-4\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m) \u001b[38;5;66;03m# learning rate\u001b[39;00m\n\u001b[1;32m    201\u001b[0m ]\n\u001b[0;32m--> 203\u001b[0m best_params, best_rmse \u001b[38;5;241m=\u001b[39m grey_wolf_optimizer(fitness_function, bounds)\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest Hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_params)\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Val RMSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, best_rmse)\n",
      "Cell \u001b[0;32mIn[5], line 174\u001b[0m, in \u001b[0;36mgrey_wolf_optimizer\u001b[0;34m(fitness_fn, bounds, wolves, iters)\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iters):\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m wolf \u001b[38;5;129;01min\u001b[39;00m pop:\n\u001b[0;32m--> 174\u001b[0m         score \u001b[38;5;241m=\u001b[39m fitness_fn(wolf)\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m<\u001b[39m a_score:\n\u001b[1;32m    176\u001b[0m             alpha, a_score \u001b[38;5;241m=\u001b[39m wolf\u001b[38;5;241m.\u001b[39mcopy(), score\n",
      "Cell \u001b[0;32mIn[5], line 134\u001b[0m, in \u001b[0;36mfitness_function\u001b[0;34m(params)\u001b[0m\n\u001b[1;32m    131\u001b[0m hidden_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(hidden_dim)\n\u001b[1;32m    132\u001b[0m num_layers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(num_layers)\n\u001b[0;32m--> 134\u001b[0m model \u001b[38;5;241m=\u001b[39m RULLSTM(\u001b[38;5;28mlen\u001b[39m(features), hidden_dim, num_layers, dropout)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m    135\u001b[0m opt \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mlr)\n\u001b[1;32m    136\u001b[0m loss_fn \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMSELoss()\n",
      "Cell \u001b[0;32mIn[5], line 110\u001b[0m, in \u001b[0;36mRULLSTM.__init__\u001b[0;34m(self, input_dim, hidden_dim, num_layers, dropout)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, input_dim, hidden_dim, num_layers, dropout):\n\u001b[1;32m    109\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m--> 110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLSTM(\n\u001b[1;32m    111\u001b[0m         input_dim, hidden_dim,\n\u001b[1;32m    112\u001b[0m         num_layers\u001b[38;5;241m=\u001b[39mnum_layers,\n\u001b[1;32m    113\u001b[0m         batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    114\u001b[0m         dropout\u001b[38;5;241m=\u001b[39mdropout\n\u001b[1;32m    115\u001b[0m     )\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\n\u001b[1;32m    117\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(hidden_dim, \u001b[38;5;241m64\u001b[39m),\n\u001b[1;32m    118\u001b[0m         nn\u001b[38;5;241m.\u001b[39mReLU(),\n\u001b[1;32m    119\u001b[0m         nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m~/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:980\u001b[0m, in \u001b[0;36mLSTM.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 980\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLSTM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Downloads/anaconda3/lib/python3.12/site-packages/torch/nn/modules/rnn.py:137\u001b[0m, in \u001b[0;36mRNNBase.__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional, proj_size, device, dtype)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhidden_size must be greater than zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_layers \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers must be greater than zero\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m proj_size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mproj_size should be a positive integer or zero to disable projections\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    141\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: num_layers must be greater than zero"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# IMPORTS\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ============================================================\n",
    "# CONFIG\n",
    "# ============================================================\n",
    "SEQ_LEN = 30\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "MAX_RUL = 125\n",
    "\n",
    "# ============================================================\n",
    "# LOAD DATA (FD001)\n",
    "# ============================================================\n",
    "cols = (\n",
    "    [\"unit\", \"cycle\"] +\n",
    "    [f\"op_{i}\" for i in range(1, 4)] +\n",
    "    [f\"s_{i}\" for i in range(1, 22)]\n",
    ")\n",
    "\n",
    "train_df = pd.read_csv(\"train_FD001.txt\", delim_whitespace=True, header=None, names=cols)\n",
    "test_df  = pd.read_csv(\"test_FD001.txt\", delim_whitespace=True, header=None, names=cols)\n",
    "rul_df   = pd.read_csv(\"RUL_FD001.txt\", header=None, names=[\"RUL\"])\n",
    "\n",
    "# ============================================================\n",
    "# RUL CALCULATION\n",
    "# ============================================================\n",
    "max_cycles = train_df.groupby(\"unit\")[\"cycle\"].max()\n",
    "train_df[\"RUL\"] = train_df.apply(\n",
    "    lambda r: max_cycles[r[\"unit\"]] - r[\"cycle\"], axis=1\n",
    ")\n",
    "train_df[\"RUL\"] = train_df[\"RUL\"].clip(upper=MAX_RUL)\n",
    "\n",
    "# ============================================================\n",
    "# FEATURE SCALING\n",
    "# ============================================================\n",
    "features = cols[2:]\n",
    "scaler = MinMaxScaler()\n",
    "train_df[features] = scaler.fit_transform(train_df[features])\n",
    "test_df[features]  = scaler.transform(test_df[features])\n",
    "\n",
    "# ============================================================\n",
    "# TRAIN / VALIDATION SPLIT (ENGINE-WISE)\n",
    "# ============================================================\n",
    "engines = train_df[\"unit\"].unique()\n",
    "train_eng, val_eng = train_test_split(engines, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df = train_df[train_df[\"unit\"].isin(train_eng)]\n",
    "val_df   = train_df[train_df[\"unit\"].isin(val_eng)]\n",
    "\n",
    "# ============================================================\n",
    "# SEQUENCE GENERATION\n",
    "# ============================================================\n",
    "def generate_sequences(df, seq_len, is_test=False):\n",
    "    X, y = [], []\n",
    "    for unit in df[\"unit\"].unique():\n",
    "        u_df = df[df[\"unit\"] == unit]\n",
    "        data = u_df[features].values\n",
    "\n",
    "        if is_test:\n",
    "            X.append(data[-seq_len:])\n",
    "        else:\n",
    "            rul = u_df[\"RUL\"].values\n",
    "            for i in range(len(data) - seq_len):\n",
    "                X.append(data[i:i + seq_len])\n",
    "                y.append(rul[i + seq_len - 1])\n",
    "\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X_train, y_train = generate_sequences(train_df, SEQ_LEN)\n",
    "X_val, y_val     = generate_sequences(val_split_df, SEQ_LEN)\n",
    "X_test, _        = generate_sequences(test_df, SEQ_LEN, is_test=True)\n",
    "\n",
    "# ============================================================\n",
    "# DATASET\n",
    "# ============================================================\n",
    "class CMAPSSDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = None if y is None else torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.X[idx], self.y[idx]) if self.y is not None else self.X[idx]\n",
    "\n",
    "train_loader = DataLoader(CMAPSSDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader   = DataLoader(CMAPSSDataset(X_val, y_val), batch_size=BATCH_SIZE)\n",
    "test_loader  = DataLoader(CMAPSSDataset(X_test), batch_size=BATCH_SIZE)\n",
    "\n",
    "# ============================================================\n",
    "# LSTM MODEL\n",
    "# ============================================================\n",
    "class RULLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return self.fc(out[:, -1]).squeeze(1)\n",
    "\n",
    "# ============================================================\n",
    "# FITNESS FUNCTION FOR GWO\n",
    "# ============================================================\n",
    "def fitness_function(params):\n",
    "    hidden_dim, num_layers, dropout, lr = params\n",
    "    hidden_dim = int(hidden_dim)\n",
    "    num_layers = max(1, int(num_layers))  # Ensure num_layers >= 1\n",
    "\n",
    "    model = RULLSTM(len(features), hidden_dim, num_layers, dropout).to(DEVICE)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    loss_fn = nn.MSELoss()\n",
    "\n",
    "    # Train few epochs for speed\n",
    "    for _ in range(5):\n",
    "        model.train()\n",
    "        for X, y in train_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            opt.zero_grad()\n",
    "            loss = loss_fn(model(X), y)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "    model.eval()\n",
    "    preds, true = [], []\n",
    "    with torch.no_grad():\n",
    "        for X, y in val_loader:\n",
    "            X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "            preds.append(model(X).cpu().numpy())\n",
    "            true.append(y.cpu().numpy())\n",
    "\n",
    "    return np.sqrt(mean_squared_error(np.concatenate(true), np.concatenate(preds)))\n",
    "\n",
    "# ============================================================\n",
    "# GREY WOLF OPTIMIZER\n",
    "# ============================================================\n",
    "def grey_wolf_optimizer(fitness_fn, bounds, wolves=6, iters=8):\n",
    "    dim = len(bounds)\n",
    "    pop = np.random.rand(wolves, dim)\n",
    "\n",
    "    for i in range(dim):\n",
    "        low, high = bounds[i]\n",
    "        pop[:, i] = low + pop[:, i] * (high - low)\n",
    "\n",
    "    alpha = beta = delta = None\n",
    "    a_score = b_score = d_score = np.inf\n",
    "\n",
    "    for t in range(iters):\n",
    "        for wolf in pop:\n",
    "            score = fitness_fn(wolf)\n",
    "            if score < a_score:\n",
    "                alpha, a_score = wolf.copy(), score\n",
    "            elif score < b_score:\n",
    "                beta, b_score = wolf.copy(), score\n",
    "            elif score < d_score:\n",
    "                delta, d_score = wolf.copy(), score\n",
    "\n",
    "        a = 2 - t * (2 / iters)\n",
    "\n",
    "        for i in range(wolves):\n",
    "            for j in range(dim):\n",
    "                X1 = alpha[j] - a * np.random.rand() * abs(alpha[j] - pop[i][j])\n",
    "                X2 = beta[j]  - a * np.random.rand() * abs(beta[j] - pop[i][j])\n",
    "                X3 = delta[j] - a * np.random.rand() * abs(delta[j] - pop[i][j])\n",
    "                pop[i][j] = (X1 + X2 + X3) / 3\n",
    "\n",
    "    return alpha, a_score\n",
    "\n",
    "# ============================================================\n",
    "# RUN GWO\n",
    "# ============================================================\n",
    "bounds = [\n",
    "    (32, 128),   # hidden_dim\n",
    "    (1, 3),      # num_layers\n",
    "    (0.1, 0.5),  # dropout\n",
    "    (1e-4, 1e-3) # learning rate\n",
    "]\n",
    "\n",
    "best_params, best_rmse = grey_wolf_optimizer(fitness_function, bounds)\n",
    "\n",
    "print(\"\\nBest Hyperparameters:\", best_params)\n",
    "print(\"Best Val RMSE:\", best_rmse)\n",
    "\n",
    "# ============================================================\n",
    "# FINAL TRAINING\n",
    "# ============================================================\n",
    "hidden, layers, dropout, lr = best_params\n",
    "model = RULLSTM(len(features), int(hidden), int(layers), dropout).to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    model.train()\n",
    "    losses = []\n",
    "    for X, y in train_loader:\n",
    "        X, y = X.to(DEVICE), y.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(model(X), y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    print(f\"Epoch {epoch+1} | Train MSE: {np.mean(losses):.4f}\")\n",
    "\n",
    "# ============================================================\n",
    "# TEST EVALUATION\n",
    "# ============================================================\n",
    "model.eval()\n",
    "preds = []\n",
    "with torch.no_grad():\n",
    "    for X in test_loader:\n",
    "        X = X.to(DEVICE)\n",
    "        preds.append(model(X).cpu().numpy())\n",
    "\n",
    "test_preds = np.concatenate(preds)\n",
    "test_rmse = np.sqrt(mean_squared_error(rul_df[\"RUL\"].values, test_preds))\n",
    "\n",
    "print(\"\\nTEST RMSE:\", test_rmse)\n",
    "\n",
    "# ============================================================\n",
    "# HEALTH DEGRADATION PLOT\n",
    "# ============================================================\n",
    "def plot_engine_health(engine_id):\n",
    "    df = train_df[train_df[\"unit\"] == engine_id]\n",
    "    health = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(df) - SEQ_LEN):\n",
    "            seq = torch.tensor(\n",
    "                df.iloc[i:i+SEQ_LEN][features].values,\n",
    "                dtype=torch.float32\n",
    "            ).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            pred = model(seq).cpu().item()\n",
    "            health.append(np.clip((pred / MAX_RUL) * 100, 0, 100))\n",
    "\n",
    "    plt.plot(health)\n",
    "    plt.xlabel(\"Cycle\")\n",
    "    plt.ylabel(\"Health (%)\")\n",
    "    plt.title(f\"Engine {engine_id} Health Degradation\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_engine_health(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7e6d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
